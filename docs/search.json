[
  {
    "objectID": "tech_hub_analysis.html",
    "href": "tech_hub_analysis.html",
    "title": "Tech Hub Analysis",
    "section": "",
    "text": "This section evaluates whether legacy tech hubs (Bay Area, Seattle, Boston, NYC, Austin) still dominate AI hiring or whether a set of emerging hubs is gaining share. We compare posting volume, AI share, remote/hybrid availability, and median salary (when available).\n\n\n\n\n\n\nNote\n\n\n\nData source: data/raw/lightcast_job_postings.csv (Lightcast, 2024; Jan–Sep if that’s what you downloaded).\nInstructions: Make sure the CSV is at data/raw/lightcast_job_postings.csv."
  },
  {
    "objectID": "tech_hub_analysis.html#purpose",
    "href": "tech_hub_analysis.html#purpose",
    "title": "Tech Hub Analysis",
    "section": "",
    "text": "This section evaluates whether legacy tech hubs (Bay Area, Seattle, Boston, NYC, Austin) still dominate AI hiring or whether a set of emerging hubs is gaining share. We compare posting volume, AI share, remote/hybrid availability, and median salary (when available).\n\n\n\n\n\n\nNote\n\n\n\nData source: data/raw/lightcast_job_postings.csv (Lightcast, 2024; Jan–Sep if that’s what you downloaded).\nInstructions: Make sure the CSV is at data/raw/lightcast_job_postings.csv."
  },
  {
    "objectID": "tech_hub_analysis.html#setup",
    "href": "tech_hub_analysis.html#setup",
    "title": "Tech Hub Analysis",
    "section": "0.2 Setup",
    "text": "0.2 Setup\nlibrary(tidyverse) library(lubridate) theme_set(theme_minimal(base_size = 14))\npath &lt;- “data/raw/lightcast_job_postings.csv” raw &lt;- read_csv(path, show_col_types = FALSE) df &lt;- raw %&gt;% rename_with(toupper) %&gt;% mutate( CITY_NAME = coalesce(CITY_NAME, CITY, NA), STATE_NAME = coalesce(STATE_NAME, STATE, NA), TITLE = coalesce(JOB_TITLE, TITLE, NA), SKILLS_TXT = coalesce(SKILLS, SKILL_TEXT, NA), POSTED = coalesce(POSTED_DATE, DATE_POSTED, as.character(NA)), POSTED = suppressWarnings(ymd(POSTED)), REMOTE_TYPE = case_when( REMOTE_TYPE %in% c(“Remote”,“REMOTE”,“remote”,“1”) ~ “Remote”, REMOTE_TYPE %in% c(“Hybrid”,“HYBRID”,“hybrid”,“3”) ~ “Hybrid”, TRUE ~ “On-site” ), SALARY = suppressWarnings(readr::parse_number(coalesce(SALARY, SALARY_TO, SALARY_FROM))) )"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "EDA",
    "section": "",
    "text": "Code\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"JobPostingsEDA\").getOrCreate()\ndf = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\") \\\n    .option(\"multiLine\",\"true\").option(\"escape\",\"\\\"\") \\\n    .csv(\"data/raw/lightcast_job_postings.csv\")\ndf.cache()\ndf.count()\n\n\n72498\n\n\n\n\nCode\nfrom pyspark.sql.functions import when, col, trim\nfrom pyspark.sql.types import IntegerType, FloatType\n\ncols_blank = [\"CITY_NAME\",\"STATE_NAME\",\"REMOTE_TYPE\",\"MIN_EDULEVELS\",\n              \"SALARY\",\"SALARY_FROM\",\"SALARY_TO\",\n              \"MIN_YEARS_EXPERIENCE\",\"MAX_YEARS_EXPERIENCE\"]\nfor c in cols_blank:\n    if c in df.columns:\n        df = df.withColumn(c, when(trim(col(c)) == \"\", None).otherwise(col(c)))\n\ndf = df.withColumn(\"REMOTE_TYPE\", col(\"REMOTE_TYPE\").cast(IntegerType())) \\\n       .withColumn(\"MIN_EDULEVELS\", col(\"MIN_EDULEVELS\").cast(IntegerType())) \\\n       .withColumn(\"SALARY\", col(\"SALARY\").cast(FloatType())) \\\n       .withColumn(\"SALARY_FROM\", col(\"SALARY_FROM\").cast(FloatType())) \\\n       .withColumn(\"SALARY_TO\", col(\"SALARY_TO\").cast(FloatType())) \\\n       .withColumn(\"MIN_YEARS_EXPERIENCE\", col(\"MIN_YEARS_EXPERIENCE\").cast(FloatType())) \\\n       .withColumn(\"MAX_YEARS_EXPERIENCE\", col(\"MAX_YEARS_EXPERIENCE\").cast(FloatType()))\n\ndf = df.withColumn(\n    \"REMOTE_TYPE\",\n    when(col(\"REMOTE_TYPE\") == 1, \"Remote\")\n    .when(col(\"REMOTE_TYPE\") == 2, \"Onsite\")\n    .when(col(\"REMOTE_TYPE\") == 3, \"Hybrid\")\n    .otherwise(\"Onsite\")\n)\n\ndf = df.withColumn(\n    \"MIN_EDULEVELS\",\n    when(col(\"MIN_EDULEVELS\").isin(0,1,99), \"Associate or lower\")\n    .when(col(\"MIN_EDULEVELS\") == 2, \"Bachelor\")\n    .when(col(\"MIN_EDULEVELS\") == 3, \"Master's\")\n    .when(col(\"MIN_EDULEVELS\") == 4, \"PhD\")\n    .otherwise(\"Associate or lower\")\n)\n\ndf = df.withColumn(\"CITY_NAME\", when(col(\"CITY_NAME\").isNull(), \"No City\").otherwise(col(\"CITY_NAME\")))\ndf = df.withColumn(\"STATE_NAME\", when(col(\"STATE_NAME\").isNull(), \"No State\").otherwise(col(\"STATE_NAME\")))\n\ndf = df.withColumn(\n    \"SALARY\",\n    when(col(\"SALARY\").isNull() & col(\"SALARY_FROM\").isNotNull() & col(\"SALARY_TO\").isNotNull(),\n         (col(\"SALARY_FROM\") + col(\"SALARY_TO\"))/2.0\n    ).otherwise(col(\"SALARY\"))\n)\ndf.count()\n\n\n72498\n\n\n\n\nCode\ndf.write.mode(\"overwrite\").parquet(\"data/clean_job_postings.parquet\")\n\n\n\n\nCode\n# Remote share by state\nstate_remote = df.groupBy(\"STATE_NAME\",\"REMOTE_TYPE\").count().orderBy(\"count\", ascending=False)\nstate_remote.show(20, truncate=False)\n\n# Top cities by postings\ndf.groupBy(\"CITY_NAME\",\"STATE_NAME\").count().orderBy(\"count\", ascending=False).show(20, truncate=False)\n\n# Salary by state\ndf.groupBy(\"STATE_NAME\").avg(\"SALARY\").orderBy(\"avg(SALARY)\", ascending=False).show(20, truncate=False)\n\n\n+--------------+-----------+-----+\n|STATE_NAME    |REMOTE_TYPE|count|\n+--------------+-----------+-----+\n|Texas         |Onsite     |6763 |\n|California    |Onsite     |5766 |\n|Florida       |Onsite     |3060 |\n|Virginia      |Onsite     |2942 |\n|Illinois      |Onsite     |2896 |\n|New York      |Onsite     |2787 |\n|North Carolina|Onsite     |2240 |\n|Ohio          |Onsite     |2198 |\n|Georgia       |Onsite     |2169 |\n|New Jersey    |Onsite     |2148 |\n|Pennsylvania  |Onsite     |1828 |\n|Massachusetts |Onsite     |1651 |\n|Michigan      |Onsite     |1522 |\n|Arizona       |Onsite     |1349 |\n|Washington    |Onsite     |1260 |\n|Colorado      |Onsite     |1141 |\n|Texas         |Remote     |1091 |\n|Minnesota     |Onsite     |1075 |\n|California    |Remote     |1056 |\n|Maryland      |Onsite     |1018 |\n+--------------+-----------+-----+\nonly showing top 20 rows\n\n\n+-----------------+---------------------------------------+-----+\n|CITY_NAME        |STATE_NAME                             |count|\n+-----------------+---------------------------------------+-----+\n|New York, NY     |New York                               |2175 |\n|Chicago, IL      |Illinois                               |1803 |\n|Atlanta, GA      |Georgia                                |1706 |\n|Austin, TX       |Texas                                  |1463 |\n|Houston, TX      |Texas                                  |1423 |\n|Dallas, TX       |Texas                                  |1326 |\n|Charlotte, NC    |North Carolina                         |1226 |\n|Washington, DC   |Washington, D.C. (District of Columbia)|1210 |\n|Boston, MA       |Massachusetts                          |1012 |\n|Richmond, VA     |Virginia                               |884  |\n|San Francisco, CA|California                             |876  |\n|Phoenix, AZ      |Arizona                                |759  |\n|Los Angeles, CA  |California                             |737  |\n|Seattle, WA      |Washington                             |650  |\n|Columbus, OH     |Ohio                                   |647  |\n|Denver, CO       |Colorado                               |630  |\n|Tampa, FL        |Florida                                |617  |\n|Minneapolis, MN  |Minnesota                              |606  |\n|Philadelphia, PA |Pennsylvania                           |595  |\n|Raleigh, NC      |North Carolina                         |568  |\n+-----------------+---------------------------------------+-----+\nonly showing top 20 rows\n\n\n+---------------------------------------+------------------+\n|STATE_NAME                             |avg(SALARY)       |\n+---------------------------------------+------------------+\n|Connecticut                            |124966.07226107226|\n|New Jersey                             |123011.95022371365|\n|Arkansas                               |122204.1050955414 |\n|Virginia                               |121915.68095572734|\n|Vermont                                |121862.91588785047|\n|California                             |121078.16375806837|\n|Montana                                |120711.40350877192|\n|Illinois                               |120562.1273692191 |\n|Washington                             |119963.5337972167 |\n|Washington, D.C. (District of Columbia)|118926.40808080808|\n|Massachusetts                          |117145.98220858896|\n|Michigan                               |116451.63358778626|\n|North Carolina                         |116420.56212424849|\n|Maryland                               |116161.16097560976|\n|Iowa                                   |115321.96656534955|\n|Oklahoma                               |114818.31034482758|\n|Minnesota                              |114782.0500736377 |\n|Texas                                  |114607.66336818521|\n|New York                               |114173.67230695901|\n|Georgia                                |114030.27155655096|\n+---------------------------------------+------------------+\nonly showing top 20 rows\n\n\n\n\nCode\nfrom pyspark.sql.functions import col, count, sum as _sum, when, to_date, year, month, desc\n\n# Parse posting date if available\ndate_col = \"POSTED_DATE\"\nif date_col in df.columns:\n    df = df.withColumn(date_col, to_date(col(date_col)))\n    df = df.withColumn(\"YEAR\", year(col(date_col))).withColumn(\"MONTH\", month(col(date_col)))\n\n# Flag AI vs non AI using title and description keywords\nai_patterns = [\"ai\",\"artificial intelligence\",\"machine learning\",\"ml\",\"deep learning\",\"nlp\",\"computer vision\",\"llm\",\"genai\",\"gen ai\",\"data scientist\",\"ml engineer\"]\ndef mk_like(c):\n    expr = None\n    for p in ai_patterns:\n        cond = col(c).rlike(f\"(?i)\\\\b{p}\\\\b\")\n        expr = cond if expr is None else (expr | cond)\n    return expr\n\nai_flag = None\nif \"TITLE\" in df.columns:\n    ai_flag = mk_like(\"TITLE\")\nif \"JOB_DESCRIPTION\" in df.columns:\n    ai_desc = mk_like(\"JOB_DESCRIPTION\")\n    ai_flag = ai_desc if ai_flag is None else (ai_flag | ai_desc)\n\nif ai_flag is not None:\n    df = df.withColumn(\"IS_AI_ROLE\", when(ai_flag, 1).otherwise(0))\nelse:\n    df = df.withColumn(\"IS_AI_ROLE\", when(col(\"TITLE\").isNotNull(), 0).otherwise(0))\n\n\n\n\nCode\nfrom pyspark.sql.functions import desc\n\n\n\nRemote share by state\nstate_remote = df.groupBy(“STATE_NAME”,“REMOTE_TYPE”).count().orderBy(“count”, ascending=False) state_remote.show(30, truncate=False)\n\n\nTop states by postings\nstate_totals = df.groupBy(“STATE_NAME”).count().orderBy(“count”, ascending=False) state_totals.show(30, truncate=False)\n\n\nAI vs non AI counts by state\nfrom pyspark.sql.functions import count, sum as _sum, avg ai_state = df.groupBy(“STATE_NAME”).agg( _sum(“IS_AI_ROLE”).alias(“ai_posts”), (count(“*“) - _sum(“IS_AI_ROLE”)).alias(“non_ai_posts”) ).orderBy(“ai_posts”, ascending=False) ai_state.show(30, truncate=False)\n\n\nMonthly trend of remote vs onsite overall\nif “YEAR” in df.columns and “MONTH” in df.columns: monthly_remote = df.groupBy(“YEAR”,“MONTH”,“REMOTE_TYPE”).count().orderBy(“YEAR”,“MONTH”,“REMOTE_TYPE”) monthly_remote.show(60, truncate=False)\n\n\nSalary by state for AI vs non AI\nif “SALARY” in df.columns: sal_by_state_ai = df.groupBy(“STATE_NAME”,“IS_AI_ROLE”).agg(avg(“SALARY”).alias(“avg_salary”)).orderBy(“avg_salary”, ascending=False) sal_by_state_ai.show(30, truncate=False) ```"
  },
  {
    "objectID": "geographic_trends.html",
    "href": "geographic_trends.html",
    "title": "Geographic Trends",
    "section": "",
    "text": "Code\nfrom pyspark.sql import SparkSession, functions as F\nimport seaborn as sns, matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\")\n\nspark = SparkSession.builder.appName(\"EDA\").getOrCreate()\ndf = spark.read.parquet(\"data/clean_job_postings.parquet\")\n\n\nOverview Geographic EDA using Lightcast cleaned Spark DataFrame df. Focus on states, remote share, salaries, and hubs.\nTop 10 States by Job postings\n\n\nCode\nstate_counts = (df.groupBy(\"STATE_NAME\")\n                  .agg(F.count(\"*\").alias(\"postings\"))\n                  .orderBy(F.col(\"postings\").desc())\n                  .limit(10))\npdf = state_counts.toPandas()\n\nplt.figure(figsize=(8,4))\nsns.barplot(data=pdf, x=\"postings\", y=\"STATE_NAME\", color=\"#4C78A8\")\nplt.title(\"Top 10 States by Job Postings\")\nplt.xlabel(\"Postings\")\nplt.ylabel(\"State\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nRemote vs Onsite Share by Industry\n\n\nCode\nremote_ind = (df.groupBy(\"NAICS2_NAME\",\"REMOTE_TYPE\")\n                .agg(F.count(\"*\").alias(\"postings\")))\npivot = (remote_ind.groupBy(\"NAICS2_NAME\")\n           .pivot(\"REMOTE_TYPE\", [\"Remote\",\"Hybrid\",\"Onsite\"])\n           .sum(\"postings\")\n           .fillna(0))\npivot = pivot.withColumn(\"total\", F.col(\"Remote\")+F.col(\"Hybrid\")+F.col(\"Onsite\")) \\\n             .withColumn(\"Remote_share\", F.col(\"Remote\")/F.col(\"total\")) \\\n             .orderBy(F.col(\"Remote_share\").desc()) \\\n             .limit(12)\npdf = pivot.select(\"NAICS2_NAME\",\"Remote_share\").toPandas()\n\nplt.figure(figsize=(8,5))\nsns.barplot(data=pdf, x=\"Remote_share\", y=\"NAICS2_NAME\", color=\"#E45756\")\nfrom matplotlib.ticker import FuncFormatter\nplt.gca().xaxis.set_major_formatter(FuncFormatter(lambda x,_: f\"{x:.0%}\"))\nplt.title(\"Industries with Highest Remote Share\")\nplt.xlabel(\"Remote share\")\nplt.ylabel(\"Industry\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAverage Salary by State\n\n\nCode\nstate_salary = (df.select(\"STATE_NAME\",\"SALARY\")\n                  .where(F.col(\"SALARY\").isNotNull())\n                  .groupBy(\"STATE_NAME\")\n                  .agg(F.count(\"*\").alias(\"n_postings\"),\n                       F.avg(\"SALARY\").alias(\"avg_salary\"),\n                       F.expr(\"percentile_approx(SALARY,0.5)\").alias(\"median_salary\"))\n                  .orderBy(F.col(\"avg_salary\").desc())\n                  .limit(20))\npdf = state_salary.toPandas()\n\nplt.figure(figsize=(9,5))\nsns.barplot(data=pdf, x=\"avg_salary\", y=\"STATE_NAME\", color=\"#72B7B2\")\nplt.title(\"Average Salary by State\")\nplt.xlabel(\"Average salary\")\nplt.ylabel(\"State\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTech Hubs vs Emerging Locations\n\n\nCode\nhub = (df.withColumn(\"hub_label\",\n          F.when(F.col(\"STATE_NAME\").isin(\"California\",\"Texas\",\"Massachusetts\",\"Washington\",\"New York\"),\n                 F.lit(\"Tech Hub\")).otherwise(F.lit(\"Emerging\")))\n         .groupBy(\"hub_label\").agg(F.count(\"*\").alias(\"postings\"))\n         .orderBy(F.col(\"postings\").desc()))\npdf = hub.toPandas()\n\nplt.figure(figsize=(5,4))\nsns.barplot(data=pdf, x=\"hub_label\", y=\"postings\", palette=\"Set2\")\nplt.title(\"Tech Hubs vs Emerging Locations\")\nplt.xlabel(\"\")\nplt.ylabel(\"Postings\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTop states by postings\n\n\nCode\ntop_states = df.groupBy(\"STATE_NAME\").count().orderBy(\"count\", ascending=False).limit(15).toPandas()\ntop_states.plot(kind=\"barh\", x=\"STATE_NAME\", y=\"count\", figsize=(8,6), legend=False, title=\"Top states by postings\")\nimport matplotlib.pyplot as plt\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nTop states by remote share\n\n\nCode\nstate_remote_pdf = df.groupBy(\"STATE_NAME\",\"REMOTE_TYPE\").count().toPandas()\npivot = state_remote_pdf.pivot_table(index=\"STATE_NAME\", columns=\"REMOTE_TYPE\", values=\"count\", aggfunc=\"sum\", fill_value=0)\npivot[\"total\"] = pivot.sum(axis=1)\nfor c in [\"Remote\",\"Hybrid\",\"Onsite\"]:\n    if c not in pivot.columns:\n        pivot[c] = 0\npivot[\"remote_share\"] = pivot[\"Remote\"] / pivot[\"total\"]\ntop_remote = pivot.sort_values(\"remote_share\", ascending=False).head(15)\n\ntop_remote[\"remote_share\"].plot(kind=\"barh\", figsize=(8,6), xlim=(0,1), title=\"Top states by remote share\")\nplt.gca().invert_yaxis()\nplt.tight_layout()"
  },
  {
    "objectID": "skill_gap_analysis.html",
    "href": "skill_gap_analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "Objective: Compare the skills required in IT job postings against the actual skills of your group members to identify knowledge gaps and areas for improvement."
  },
  {
    "objectID": "skill_gap_analysis.html#create-a-team-based-skill-dataframe",
    "href": "skill_gap_analysis.html#create-a-team-based-skill-dataframe",
    "title": "Skill Gap Analysis",
    "section": "1 Create a team-based skill dataframe",
    "text": "1 Create a team-based skill dataframe\nEach team member should list their current skills relevant to their selected IT career path. Use a numerical scale (1-5) to indicate proficiency levels:\nNote: Each team member has been given a fictional score to better demonstrate the variations in proficiency level.\n\n1 = Beginner\n2 = Basic knowledge\n3 = Intermediate\n4 = Advanced\n5 = Expert\n\n\n\nCode\nimport pandas as pd\n\nskills_data = {\n    \"Name\": [\"Thomas\", \"Fayobomi\", \"Dominique\", \"Aryan\"],\n    \"Python\": [1, 2, 4, 5],\n    \"SQL\": [2, 3, 3, 4],\n    \"Machine Learning\": [1, 2, 4, 4],\n    \"Cloud Computing\": [1, 2, 3, 5]\n}\n\ndf_skills = pd.DataFrame(skills_data)\ndf_skills.set_index(\"Name\", inplace=True)\ndf_skills\n\n\n\n\n\n\n\n\n\nPython\nSQL\nMachine Learning\nCloud Computing\n\n\nName\n\n\n\n\n\n\n\n\nThomas\n1\n2\n1\n1\n\n\nFayobomi\n2\n3\n2\n2\n\n\nDominique\n4\n3\n4\n3\n\n\nAryan\n5\n4\n4\n5\n\n\n\n\n\n\n\nYou can use heatmaps or any other visualizations to visualize team strengths and gaps.\n\n1.1 \nVisualizing Skill Gaps with Seaborn\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(df_skills, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Team Skill Levels Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n1.2 Compare team skills to industry requirements\nExtract most in-demand skills from IT job postings. Identify gaps between team skill levels and job expectations."
  },
  {
    "objectID": "skill_gap_analysis.html#extracting-top-skills-from-job-descriptions",
    "href": "skill_gap_analysis.html#extracting-top-skills-from-job-descriptions",
    "title": "Skill Gap Analysis",
    "section": "2 Extracting Top Skills from Job Descriptions",
    "text": "2 Extracting Top Skills from Job Descriptions\n\n\nCode\nfrom collections import Counter\n\n#Assuming job_descriptions is a list of text from job postings\ntop_skills = [\"Python\", \"SQL\", \"Machine Learning\", \"Cloud Computing\", \"Docker\", \"AWS\"]\njob_skill_counts = Counter(top_skills)\n\n# Compare with team skill levels\nfor skill in top_skills:\n    if skill not in df_skills.columns:\n        df_skills[skill] = 0  # Assume no knowledge in missing skills\n\ndf_skills\n\n\n\n\n\n\n\n\n\nPython\nSQL\nMachine Learning\nCloud Computing\nDocker\nAWS\n\n\nName\n\n\n\n\n\n\n\n\n\n\nThomas\n1\n2\n1\n1\n0\n0\n\n\nFayobomi\n2\n3\n2\n2\n0\n0\n\n\nDominique\n4\n3\n4\n3\n0\n0\n\n\nAryan\n5\n4\n4\n5\n0\n0"
  },
  {
    "objectID": "skill_gap_analysis.html#improvement-plan",
    "href": "skill_gap_analysis.html#improvement-plan",
    "title": "Skill Gap Analysis",
    "section": "3 Improvement Plan",
    "text": "3 Improvement Plan\nWhich skills should each member prioritize learning? What courses or resources can help? How can the team collaborate to bridge skill gaps?"
  },
  {
    "objectID": "RandomForest.html",
    "href": "RandomForest.html",
    "title": "Geographic & Remote Work Analysis",
    "section": "",
    "text": "Regression Analysis\n\n\nCode\nfrom pyspark.sql import SparkSession\n\n# Start a Spark session\nspark = SparkSession.builder.appName(\"JobPostingsAnalysis\").getOrCreate()\n\n# Load the CSV file into a Spark DataFrame\ndf = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"multiLine\",\"true\").option(\"escape\", \"\\\"\").csv(\"data/raw/lightcast_job_postings.csv\")\n\n# Register the DataFrame as a temporary SQL view\ndf.createOrReplaceTempView(\"job_postings\")\n\n#df.show(5)\n\n\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/10/14 16:29:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[Stage 1:&gt;                                                          (0 + 1) / 1]                                                                                25/10/14 16:29:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n\n\nData Preparation & Cleaning\n\n\nCode\n# Cleaning REMOTE_TYPE_NAME column\n# Missing values are replaced\n\nfrom pyspark.sql.functions import when, col, trim\n\ndf = df.withColumn(\n    \"REMOTE_TYPE\",\n    when(col(\"REMOTE_TYPE\") == 3, \"Hybrid\")\n    .when(col(\"REMOTE_TYPE\") == 1, \"Remote\")\n    .when(col(\"REMOTE_TYPE\") == 2, \"Onsite\")\n    .otherwise(\"Onsite\")\n)\n\ndf = df.withColumn(\n    \"MIN_EDULEVELS\",\n    when(col(\"MIN_EDULEVELS\") == 99, \"Associate or lower\")\n    .when(col(\"MIN_EDULEVELS\") == 0, \"Associate or lower\")\n    .when(col(\"MIN_EDULEVELS\") == 1, \"Associate or lower\")\n    .when(col(\"MIN_EDULEVELS\") == 2, \"Bachelor\")\n    .when(col(\"MIN_EDULEVELS\") == 3, \"Master's\")\n    .when(col(\"MIN_EDULEVELS\") == 4, \"PhD\")\n    .otherwise(\"Associate or lower\")\n)\n\ndf = df.withColumn(\n    \"CITY_NAME\",\n    when((col(\"CITY_NAME\").isNull()),\"No City\")\n    .otherwise(col(\"CITY_NAME\"))\n)\n\ndf = df.withColumn(\n    \"STATE_NAME\",\n    when((col(\"STATE_NAME\").isNull()),\"No City\")\n    .otherwise(col(\"STATE_NAME\"))\n)\n\n\n\n\nCode\n# Ensure numeric columns are of float type\n\ndf = df.withColumn(\"SALARY\", col(\"SALARY\").cast(\"float\")) \\\n    .withColumn(\"SALARY_FROM\", col(\"SALARY_FROM\").cast(\"float\")) \\\n    .withColumn(\"SALARY_TO\", col(\"SALARY_TO\").cast(\"float\")) \\\n    .withColumn(\"MIN_YEARS_EXPERIENCE\", col(\"MIN_YEARS_EXPERIENCE\").cast(\"float\")) \\\n    .withColumn(\"MAX_YEARS_EXPERIENCE\", col(\"MAX_YEARS_EXPERIENCE\").cast(\"float\"))\n\n\n\n\nCode\n# Removing any remaining NA values and selecting relevant columns for random forest analysis\n\nfrom pyspark.sql.functions import col, pow\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\nfrom pyspark.ml import Pipeline\n\n# Drop the NA values in relevant columns\nrf_df = df.dropna(subset=[\n    \"MIN_YEARS_EXPERIENCE\",\n    \"SALARY\", \n    \"REMOTE_TYPE\",\n    \"STATE_NAME\",\n    \"MIN_EDULEVELS\",\n    \"NAICS2_NAME\",\n]).select(\n    \"MIN_YEARS_EXPERIENCE\",\n    \"SALARY\", \n    \"REMOTE_TYPE\",\n    \"STATE_NAME\",\n    \"MIN_EDULEVELS\",\n    \"NAICS2_NAME\",\n)\n\n\nWhy select these variables?\nThe goal of using these variables is to be able to identify how much wieght/impact locations (such as state) has on the salary, in comparison to other variables such as years of experience.\nRandom Forest Model Deployment\n\n\nCode\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import DoubleType\n\n# Use only categoricals\ncategorical_columns = [\"REMOTE_TYPE\", \"STATE_NAME\", \"MIN_EDULEVELS\", \"NAICS2_NAME\"]\n\n# Index -&gt; OHE -&gt; Assemble\nindexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n            for c in categorical_columns]\nencoders = [OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_ohe\")\n            for c in categorical_columns]\nassembler = VectorAssembler(inputCols=[f\"{c}_ohe\" for c in categorical_columns],\n                            outputCol=\"features\")\n\n# Model\nrf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"SALARY\",\n                           numTrees=300, maxDepth=12, seed=42)\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n\n\n\n\nCode\nrf = RandomForestRegressor(\n    featuresCol=\"features\",\n    labelCol=\"SALARY\",\n    numTrees=300,\n    maxDepth=12,\n    seed=42\n)\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n\n#Train / Test Split - Using 80% of data for training and 20% for testing\ntrain_df, test_df = rf_df.randomSplit([0.8, 0.2], seed=42)\n\n# Fit and Predict\nmodel = pipeline.fit(train_df)\npred = model.transform(test_df)\n\n# RMSE, MAE, R squared\nfor metric in [\"rmse\", \"mae\", \"r2\"]:\n    ev = RegressionEvaluator(labelCol=\"SALARY\", predictionCol=\"prediction\", metricName=metric)\n    print(metric.upper(), \"=\", ev.evaluate(pred))\n\n\n[Stage 2:&gt;                                                          (0 + 1) / 1]                                                                                [Stage 8:&gt;                                                          (0 + 1) / 1]                                                                                [Stage 14:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 20:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 26:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 27:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 28:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 30:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 32:&gt;                                                         (0 + 1) / 1]                                                                                25/10/14 16:30:48 WARN DAGScheduler: Broadcasting large task binary with size 1118.7 KiB\n[Stage 34:&gt;                                                         (0 + 1) / 1]                                                                                25/10/14 16:30:53 WARN DAGScheduler: Broadcasting large task binary with size 2009.6 KiB\n[Stage 36:&gt;                                                         (0 + 1) / 1][Stage 37:&gt;                                                         (0 + 1) / 1]                                                                                25/10/14 16:31:01 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n[Stage 38:&gt;                                                         (0 + 1) / 1][Stage 39:&gt;                                                         (0 + 1) / 1]                                                                                25/10/14 16:31:09 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n[Stage 40:&gt;                                                         (0 + 1) / 1][Stage 41:&gt;                                                         (0 + 1) / 1]                                                                                25/10/14 16:31:21 WARN DAGScheduler: Broadcasting large task binary with size 7.2 MiB\n[Stage 42:&gt;                                                         (0 + 1) / 1]25/10/14 16:31:34 WARN DAGScheduler: Broadcasting large task binary with size 1026.2 KiB\n[Stage 43:&gt;                                                         (0 + 1) / 1]                                                                                25/10/14 16:31:36 WARN DAGScheduler: Broadcasting large task binary with size 9.8 MiB\n[Stage 44:&gt;                                                         (0 + 1) / 1]25/10/14 16:31:48 WARN DAGScheduler: Broadcasting large task binary with size 1203.7 KiB\n[Stage 45:&gt;                                                         (0 + 1) / 1]                                                                                25/10/14 16:31:51 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n[Stage 46:&gt;                                                         (0 + 1) / 1]25/10/14 16:32:03 WARN DAGScheduler: Broadcasting large task binary with size 1389.0 KiB\n[Stage 47:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 48:&gt;                                                         (0 + 0) / 1]25/10/14 16:32:05 WARN DAGScheduler: Broadcasting large task binary with size 15.9 MiB\n[Stage 48:&gt;                                                         (0 + 1) / 1]25/10/14 16:32:22 WARN DAGScheduler: Broadcasting large task binary with size 1551.4 KiB\n[Stage 49:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 50:&gt;                                                         (0 + 0) / 1]25/10/14 16:32:25 WARN DAGScheduler: Broadcasting large task binary with size 19.4 MiB\n[Stage 50:&gt;                                                         (0 + 1) / 1]25/10/14 16:32:43 WARN DAGScheduler: Broadcasting large task binary with size 1716.3 KiB\n[Stage 51:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 52:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\nRMSE = 38837.40532573633\n\n\n[Stage 53:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\nMAE = 30066.577004432453\n\n\n[Stage 54:&gt;                                                         (0 + 1) / 1]\n\n\nR2 = 0.19334552245056114\n\n\n                                                                                \n\n\nInterpretation\nThese variables collectively explain the salary by about 20%. Whereas in the testing data, the tested salaries were off by ~39K compared to the actuals.\nFeature Importance\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nrf_stage = model.stages[-1]\nimportances = np.array(rf_stage.featureImportances.toArray())\n\nmeta = pred.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"]\nattrs = sorted([*meta.get(\"binary\", []), *meta.get(\"numeric\", [])], key=lambda a: a[\"idx\"])\nfeature_names = [a[\"name\"] for a in attrs]\n\ntopn = 20\nfeat_imp = (pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n              .sort_values(\"Importance\", ascending=False)\n              .head(topn))\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=feat_imp, x=\"Importance\", y=\"Feature\")\nplt.title(f\"Top {topn} Random Forest Feature Importances\")\nplt.xlabel(\"Importance\"); plt.ylabel(\"Feature\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nDespite a relatively lower R squared value below 20% - this chart clearly shows that state (and ultimately location) has less influence on the amount of salary an employee receives compared to their education, position, and field. These three factors can be summarized as the amount of skill an employee has, and how much demand is required for said skill.\nPredictive Analysis (Extra Credit)\nThis analysis uses test/fake data to estimate what the Random Forest model would predict the salaries to be. There are 10 different variations of job type, state, education, etc. to predict each salary.\n\n\nCode\nfrom pyspark.sql import functions as F\n\ncat_cols = [\"REMOTE_TYPE\", \"STATE_NAME\", \"MIN_EDULEVELS\", \"NAICS2_NAME\"]\nnum_cols = [\"MIN_YEARS_EXPERIENCE\"]\n\n# Generic data to test predictive analysis\ntest_data = [\n    {\"REMOTE_TYPE\": \"Onsite\", \"STATE_NAME\": \"California\", \"MIN_EDULEVELS\": \"Master's\",\n     \"NAICS2_NAME\": \"Information\", \"MIN_YEARS_EXPERIENCE\": 7.0},\n\n    {\"REMOTE_TYPE\": \"Remote\", \"STATE_NAME\": \"Texas\", \"MIN_EDULEVELS\": \"Associate or lower\",\n     \"NAICS2_NAME\": \"Administrative and Support and Waste Management and Remediation Services\", \"MIN_YEARS_EXPERIENCE\": 2.0},\n\n    {\"REMOTE_TYPE\": \"Onsite\", \"STATE_NAME\": \"Washington\", \"MIN_EDULEVELS\": \"Bachelor\",\n     \"NAICS2_NAME\": \"Professional, Scientific, and Technical Services\", \"MIN_YEARS_EXPERIENCE\": 12.0},\n\n    {\"REMOTE_TYPE\": \"Hybrid\", \"STATE_NAME\": \"Illinois\", \"MIN_EDULEVELS\": \"Bachelor\",\n     \"NAICS2_NAME\": \"Finance and Insurance\", \"MIN_YEARS_EXPERIENCE\": 6.0},\n\n    {\"REMOTE_TYPE\": \"Onsite\", \"STATE_NAME\": \"New York\", \"MIN_EDULEVELS\": \"Master's\",\n     \"NAICS2_NAME\": \"Educational Services\", \"MIN_YEARS_EXPERIENCE\": 10.0},\n\n    {\"REMOTE_TYPE\": \"Hybrid\", \"STATE_NAME\": \"Florida\", \"MIN_EDULEVELS\": \"Bachelor\",\n     \"NAICS2_NAME\": \"Manufacturing\", \"MIN_YEARS_EXPERIENCE\": 8.0},\n\n    {\"REMOTE_TYPE\": \"Onsite\", \"STATE_NAME\": \"Ohio\", \"MIN_EDULEVELS\": \"Associate or lower\",\n     \"NAICS2_NAME\": \"Health Care and Social Assistance\", \"MIN_YEARS_EXPERIENCE\": 3.0},\n\n    {\"REMOTE_TYPE\": \"Onsite\", \"STATE_NAME\": \"Virginia\", \"MIN_EDULEVELS\": \"PhD\",\n     \"NAICS2_NAME\": \"Professional, Scientific, and Technical Services\", \"MIN_YEARS_EXPERIENCE\": 9.0},\n\n    {\"REMOTE_TYPE\": \"Remote\", \"STATE_NAME\": \"Colorado\", \"MIN_EDULEVELS\": \"Bachelor\",\n     \"NAICS2_NAME\": \"Real Estate and Rental and Leasing\", \"MIN_YEARS_EXPERIENCE\": 7.0},\n\n    {\"REMOTE_TYPE\": \"Onsite\", \"STATE_NAME\": \"Michigan\", \"MIN_EDULEVELS\": \"Bachelor\",\n     \"NAICS2_NAME\": \"Wholesale Trade\", \"MIN_YEARS_EXPERIENCE\": 5.0},\n]\n\nnew_df = spark.createDataFrame(test_data)\n\n# Preventative for nulls in categoricals your pipeline indexes\nnew_df = new_df.fillna({c: \"Unknown\" for c in cat_cols})\n\n# Predicting the salaries using trained pipeline model\npred = model.transform(new_df)\n\npred.select(*cat_cols, *num_cols, F.col(\"prediction\").alias(\"predicted_salary\")).show(truncate=False)\n\n\n[Stage 55:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n+-----------+----------+------------------+------------------------------------------------------------------------+--------------------+------------------+\n|REMOTE_TYPE|STATE_NAME|MIN_EDULEVELS     |NAICS2_NAME                                                             |MIN_YEARS_EXPERIENCE|predicted_salary  |\n+-----------+----------+------------------+------------------------------------------------------------------------+--------------------+------------------+\n|Onsite     |California|Master's          |Information                                                             |7.0                 |170894.85297213428|\n|Remote     |Texas     |Associate or lower|Administrative and Support and Waste Management and Remediation Services|2.0                 |114544.86580019069|\n|Onsite     |Washington|Bachelor          |Professional, Scientific, and Technical Services                        |12.0                |140036.93253450037|\n|Hybrid     |Illinois  |Bachelor          |Finance and Insurance                                                   |6.0                 |121486.09323532102|\n|Onsite     |New York  |Master's          |Educational Services                                                    |10.0                |75674.94831376836 |\n|Hybrid     |Florida   |Bachelor          |Manufacturing                                                           |8.0                 |123385.13022711083|\n|Onsite     |Ohio      |Associate or lower|Health Care and Social Assistance                                       |3.0                 |102082.35197408112|\n|Onsite     |Virginia  |PhD               |Professional, Scientific, and Technical Services                        |9.0                 |230041.21206285967|\n|Remote     |Colorado  |Bachelor          |Real Estate and Rental and Leasing                                      |7.0                 |111828.60134917914|\n|Onsite     |Michigan  |Bachelor          |Wholesale Trade                                                         |5.0                 |113658.3199482272 |\n+-----------+----------+------------------+------------------------------------------------------------------------+--------------------+------------------+\n\n\n\nThe results are varied - in some instances the results align with our previous analysis - higher eduction, onsite, and tech should see higher salaries. For example, the first line and eigth line do have the highest salaries being in information and tech alongside 5+ years of experience, onsite, and higher education completed. However, the fith line has the lowest salary, but does not necessarily align in an intuitive sense - 10 years of experience, Masters, onsite, etc. should be ranked higher.\nWhile the predictive power does show, fine tuning and additional model improvements can be made to improve the predictive capabilities.\nClustering Analysis\nThis clustering analysis explores salary and years of experience by the NAICS Name\n\n\nCode\n# KMEANS by NAICS NAME\nfrom pyspark.sql import functions as F\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\nlabel_col = \"NAICS2_NAME\"\nnumeric_features = [\"SALARY\", \"MIN_YEARS_EXPERIENCE\"]\n\n# Prepare data\nbase = (\n    rf_df.select(*(numeric_features + [label_col]))\n         .withColumn(\"SALARY\", F.col(\"SALARY\").cast(\"double\"))\n         .withColumn(\"MIN_YEARS_EXPERIENCE\", F.col(\"MIN_YEARS_EXPERIENCE\").cast(\"double\"))\n         .na.drop(subset=numeric_features + [label_col])\n)\n\n# Using the log of salary to reduce skew and defining features before using them\nbase = base.withColumn(\"log_salary\", F.log1p(F.col(\"SALARY\")))\nfeatures_for_model = [\"log_salary\", \"MIN_YEARS_EXPERIENCE\"]\n\nassembler = VectorAssembler(inputCols=features_for_model, outputCol=\"features_raw\")\nscaler    = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n\n# Finding the best k by silhouette method\nlabel_count = base.select(label_col).distinct().count()\nk_candidates = sorted({2,3,4,5,6,7,8, label_count})\n\nevalr = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"prediction\", metricName=\"silhouette\")\nbest_score, best_model = -1.0, None\nfor k in [k for k in k_candidates if k &gt;= 3]:\n    km   = KMeans(k=k, seed=42, featuresCol=\"features\", predictionCol=\"prediction\")\n    pipe = Pipeline(stages=[assembler, scaler, km]).fit(base)\n    score = evalr.evaluate(pipe.transform(base))\n    if score &gt; best_score:\n        best_score, best_model = score, pipe\n\nprint(f\"[Silhouette] k={best_model.stages[-1].getK()}  score={best_score:.3f}\")\n\n# Prediction\npred = best_model.transform(base).withColumnRenamed(\"prediction\", \"cluster\")\n\npdf = pred.select(\"cluster\", label_col, *features_for_model).toPandas()\n\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, homogeneity_score, completeness_score, v_measure_score\nimport numpy as np, pandas as pd\n\ny_c = pdf[\"cluster\"].to_numpy()\ny_l = pdf[label_col].astype(str).to_numpy()\n\nct = pd.crosstab(y_c, y_l)         # purity\npurity = ct.max(axis=1).sum() / ct.values.sum()\n\nprint(\"\\n[Cluster → top labels]\")\nfor cl, row in ct.iterrows():\n    top = row.sort_values(ascending=False).head(5)\n    print(f\"Cluster {cl}: \" + \", \".join([f\"{k}:{v}\" for k,v in top.items()]))\n\n# Summarized data per cluster   \npred.groupBy(\"cluster\").agg(\n    F.count(\"*\").alias(\"n\"),\n    F.avg(\"SALARY\").alias(\"avg_salary\"),\n    F.avg(\"MIN_YEARS_EXPERIENCE\").alias(\"avg_min_years_exp\")\n).orderBy(\"cluster\").show()\n\n# Visualizing the results\nplot_pdf = pred.select(\"MIN_YEARS_EXPERIENCE\", \"SALARY\", \"cluster\") \\\n               .sample(False, 0.25, 42).toPandas()\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,6))\ns = plt.scatter(plot_pdf[\"MIN_YEARS_EXPERIENCE\"], plot_pdf[\"SALARY\"], c=plot_pdf[\"cluster\"], alpha=.6)\nplt.xlabel(\"MIN_YEARS_EXPERIENCE\"); plt.ylabel(\"SALARY\")\nplt.title(\"Clustering by NAICS2\")\nhandles,_ = s.legend_elements()\nplt.legend(handles, [f\"Cluster {i}\" for i in sorted(plot_pdf['cluster'].unique())], title=\"Clusters\")\nplt.grid(True, alpha=.3); plt.tight_layout(); plt.show()\n\n\n[Stage 57:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 63:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 66:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 69:&gt;                                                         (0 + 1) / 1]                                                                                [Stage 117:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 120:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 122:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 125:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 128:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 131:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 179:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 182:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 184:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 187:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 190:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 193:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 241:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 244:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 246:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 249:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 252:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 255:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 303:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 306:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 308:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 311:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 314:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 317:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 361:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 364:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 366:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 369:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 372:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 375:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 407:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 410:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 412:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 415:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 418:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 421:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 469:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 472:&gt;                                                        (0 + 1) / 1]                                                                                [Stage 474:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n[Silhouette] k=3  score=0.575\n\n\n[Stage 477:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n\n[Cluster → top labels]\nCluster 0: Professional, Scientific, and Technical Services:2722, Unclassified Industry:710, Finance and Insurance:702, Information:514, Administrative and Support and Waste Management and Remediation Services:462\nCluster 1: Professional, Scientific, and Technical Services:1212, Administrative and Support and Waste Management and Remediation Services:1073, Unclassified Industry:880, Finance and Insurance:826, Educational Services:563\nCluster 2: Professional, Scientific, and Technical Services:3270, Finance and Insurance:1577, Administrative and Support and Waste Management and Remediation Services:1093, Information:1019, Unclassified Industry:850\n\n\n[Stage 478:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n+-------+-----+------------------+------------------+\n|cluster|    n|        avg_salary| avg_min_years_exp|\n+-------+-----+------------------+------------------+\n|      0| 6348|156468.69848771265|10.139729048519218|\n|      1| 6997| 74872.70701729313|2.8339288266399887|\n|      2|10352| 133393.5200927357|  4.50396058732612|\n+-------+-----+------------------+------------------+\n\n\n\n[Stage 481:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n\n\n\n\n\n\n\nThe KMeans cluster mostly uses salaries to seperate each cluster group - while some clusters pay lower or higher pay at the same years of experience, this suggests that the industry does influence salaries at different levels of experience. Overall, salaries generally rise with experience, though industry effects can override that pattern.\nAdditional Analytics\nThis section is to provide a quick salary state comparison\n\n\nCode\nfrom pyspark.sql import functions as F\n\n# Average salary by state\nstate_salary = (\n    rf_df\n      .select(\"STATE_NAME\", \"SALARY\")\n      .withColumn(\"SALARY\", F.col(\"SALARY\").cast(\"double\"))\n      .na.drop(subset=[\"STATE_NAME\", \"SALARY\"])\n      .groupBy(\"STATE_NAME\")\n      .agg(\n          F.count(\"*\").alias(\"n_postings\"),\n          F.avg(\"SALARY\").alias(\"avg_salary\"),\n          F.expr(\"percentile_approx(SALARY, 0.5)\").alias(\"median_salary\")\n      )\n      .orderBy(F.col(\"avg_salary\").desc())\n)\nstate_salary.show(60, truncate=False)\n\n# Pandas table\ntbl = (\n    state_salary\n      .withColumn(\"avg_salary\", F.round(\"avg_salary\", 0))\n      .withColumn(\"median_salary\", F.round(\"median_salary\", 0))\n      .orderBy(F.col(\"avg_salary\").desc())\n      .toPandas()\n)\nprint(tbl)\n\n\n[Stage 482:&gt;                                                        (0 + 1) / 1]                                                                                \n\n\n+---------------------------------------+----------+------------------+-------------+\n|STATE_NAME                             |n_postings|avg_salary        |median_salary|\n+---------------------------------------+----------+------------------+-------------+\n|Connecticut                            |337       |132311.49554896142|128250.0     |\n|Vermont                                |76        |130313.26315789473|121300.0     |\n|New Jersey                             |639       |129825.1392801252 |127000.0     |\n|California                             |2905      |129321.2464716007 |125900.0     |\n|Delaware                               |131       |128099.2824427481 |124497.0     |\n|Virginia                               |1039      |127059.95765158806|124900.0     |\n|Washington                             |736       |127048.63451086957|122500.0     |\n|Arkansas                               |255       |126950.65098039215|122195.0     |\n|Massachusetts                          |591       |125961.12351945855|119500.0     |\n|Illinois                               |959       |125788.52137643378|123850.0     |\n|Nebraska                               |163       |125678.96319018405|123850.0     |\n|Washington, D.C. (District of Columbia)|390       |124774.45128205128|118269.0     |\n|Michigan                               |569       |124122.14411247802|125000.0     |\n|Wisconsin                              |295       |123732.62033898305|123850.0     |\n|Montana                                |86        |123239.25581395348|119240.0     |\n|Georgia                                |624       |122651.26282051283|122195.0     |\n|Maryland                               |484       |122621.85537190082|117500.0     |\n|North Carolina                         |737       |122430.81818181818|123850.0     |\n|Iowa                                   |251       |121902.98804780876|118500.0     |\n|Texas                                  |1975      |121879.06481012658|120000.0     |\n|Kansas                                 |260       |121805.66923076923|116500.0     |\n|Oklahoma                               |249       |120403.7469879518 |120425.0     |\n|Pennsylvania                           |573       |120339.40837696336|118500.0     |\n|New York                               |1476      |120249.52574525746|113100.0     |\n|Missouri                               |424       |120182.69103773584|117500.0     |\n|Minnesota                              |507       |120115.28007889546|117500.0     |\n|Idaho                                  |197       |119481.64467005077|120425.0     |\n|South Carolina                         |204       |119466.04901960785|112800.0     |\n|Florida                                |1147      |119453.30601569312|119350.0     |\n|Wyoming                                |51        |119226.49019607843|107000.0     |\n|Kentucky                               |211       |119131.01421800948|110700.0     |\n|Maine                                  |141       |119006.82269503546|110000.0     |\n|Rhode Island                           |171       |118844.18713450292|114400.0     |\n|Ohio                                   |708       |118742.20338983051|118676.0     |\n|Alabama                                |210       |118709.1761904762 |113520.0     |\n|Indiana                                |300       |118553.04666666666|116448.0     |\n|Tennessee                              |370       |118529.64864864865|116500.0     |\n|Louisiana                              |153       |118171.94117647059|110685.0     |\n|Hawaii                                 |120       |117889.175        |110155.0     |\n|New Hampshire                          |113       |116386.54867256636|104884.0     |\n|Arizona                                |577       |115707.33275563258|118000.0     |\n|Colorado                               |738       |115357.58943089431|112382.0     |\n|North Dakota                           |61        |113977.14754098361|105144.0     |\n|Oregon                                 |510       |113884.91568627451|111370.0     |\n|Utah                                   |216       |113842.82407407407|110685.0     |\n|South Dakota                           |124       |110784.75         |100300.0     |\n|West Virginia                          |49        |109530.10204081633|96750.0      |\n|Alaska                                 |113       |108910.82300884956|95450.0      |\n|Mississippi                            |147       |107830.13605442178|96750.0      |\n|Nevada                                 |229       |104913.13973799127|102700.0     |\n|New Mexico                             |106       |104371.09433962264|96750.0      |\n+---------------------------------------+----------+------------------+-------------+\n\n\n\n[Stage 485:&gt;                                                        (0 + 1) / 1]\n\n\n                                 STATE_NAME  n_postings  avg_salary  \\\n0                               Connecticut         337    132311.0   \n1                                   Vermont          76    130313.0   \n2                                New Jersey         639    129825.0   \n3                                California        2905    129321.0   \n4                                  Delaware         131    128099.0   \n5                                  Virginia        1039    127060.0   \n6                                Washington         736    127049.0   \n7                                  Arkansas         255    126951.0   \n8                             Massachusetts         591    125961.0   \n9                                  Illinois         959    125789.0   \n10                                 Nebraska         163    125679.0   \n11  Washington, D.C. (District of Columbia)         390    124774.0   \n12                                 Michigan         569    124122.0   \n13                                Wisconsin         295    123733.0   \n14                                  Montana          86    123239.0   \n15                                  Georgia         624    122651.0   \n16                                 Maryland         484    122622.0   \n17                           North Carolina         737    122431.0   \n18                                     Iowa         251    121903.0   \n19                                    Texas        1975    121879.0   \n20                                   Kansas         260    121806.0   \n21                                 Oklahoma         249    120404.0   \n22                             Pennsylvania         573    120339.0   \n23                                 New York        1476    120250.0   \n24                                 Missouri         424    120183.0   \n25                                Minnesota         507    120115.0   \n26                                    Idaho         197    119482.0   \n27                           South Carolina         204    119466.0   \n28                                  Florida        1147    119453.0   \n29                                  Wyoming          51    119226.0   \n30                                 Kentucky         211    119131.0   \n31                                    Maine         141    119007.0   \n32                             Rhode Island         171    118844.0   \n33                                     Ohio         708    118742.0   \n34                                  Alabama         210    118709.0   \n35                                  Indiana         300    118553.0   \n36                                Tennessee         370    118530.0   \n37                                Louisiana         153    118172.0   \n38                                   Hawaii         120    117889.0   \n39                            New Hampshire         113    116387.0   \n40                                  Arizona         577    115707.0   \n41                                 Colorado         738    115358.0   \n42                             North Dakota          61    113977.0   \n43                                   Oregon         510    113885.0   \n44                                     Utah         216    113843.0   \n45                             South Dakota         124    110785.0   \n46                            West Virginia          49    109530.0   \n47                                   Alaska         113    108911.0   \n48                              Mississippi         147    107830.0   \n49                                   Nevada         229    104913.0   \n50                               New Mexico         106    104371.0   \n\n    median_salary  \n0        128250.0  \n1        121300.0  \n2        127000.0  \n3        125900.0  \n4        124497.0  \n5        124900.0  \n6        122500.0  \n7        122195.0  \n8        119500.0  \n9        123850.0  \n10       123850.0  \n11       118269.0  \n12       125000.0  \n13       123850.0  \n14       119240.0  \n15       122195.0  \n16       117500.0  \n17       123850.0  \n18       118500.0  \n19       120000.0  \n20       116500.0  \n21       120425.0  \n22       118500.0  \n23       113100.0  \n24       117500.0  \n25       117500.0  \n26       120425.0  \n27       112800.0  \n28       119350.0  \n29       107000.0  \n30       110700.0  \n31       110000.0  \n32       114400.0  \n33       118676.0  \n34       113520.0  \n35       116448.0  \n36       116500.0  \n37       110685.0  \n38       110155.0  \n39       104884.0  \n40       118000.0  \n41       112382.0  \n42       105144.0  \n43       111370.0  \n44       110685.0  \n45       100300.0  \n46        96750.0  \n47        95450.0  \n48        96750.0  \n49       102700.0  \n50        96750.0"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Research Introduction: AI vs. Non-AI Careers",
    "section": "",
    "text": "The American labor market is undergoing a fundamental geographic restructuring. Factors such as the COVID-19 global pandemic have shifted the way businesses operate, the rise of Artificial Intelligence (AI) continuously impacts the labor force as it improves, and emerging tech hubs have presented competition for established tech hubs. These modern complexities raise critical questions:\n\nWhich cities or states have the highest job growth for AI vs. non-AI careers?\nAre remote jobs increasing or decreasing across industries?\nDo tech hubs still dominate hiring, or are other locations emerging?\nHow do urban vs. rural job markets differ for AI and non-AI careers?\n\nThis research examines job growth patterns, remote work trends, and the shifting geography of AI and non-AI careers to understand how location shapes opportunity in the evolving American economy."
  },
  {
    "objectID": "introduction.html#introduction",
    "href": "introduction.html#introduction",
    "title": "Research Introduction: AI vs. Non-AI Careers",
    "section": "",
    "text": "The American labor market is undergoing a fundamental geographic restructuring. Factors such as the COVID-19 global pandemic have shifted the way businesses operate, the rise of Artificial Intelligence (AI) continuously impacts the labor force as it improves, and emerging tech hubs have presented competition for established tech hubs. These modern complexities raise critical questions:\n\nWhich cities or states have the highest job growth for AI vs. non-AI careers?\nAre remote jobs increasing or decreasing across industries?\nDo tech hubs still dominate hiring, or are other locations emerging?\nHow do urban vs. rural job markets differ for AI and non-AI careers?\n\nThis research examines job growth patterns, remote work trends, and the shifting geography of AI and non-AI careers to understand how location shapes opportunity in the evolving American economy."
  },
  {
    "objectID": "introduction.html#research-rationale",
    "href": "introduction.html#research-rationale",
    "title": "Research Introduction: AI vs. Non-AI Careers",
    "section": "Research Rationale",
    "text": "Research Rationale\n\nAI’s Impact on Employment Geography\nBessen ((2018)) argues that AI’s employment impact depends critically on market demand rather than automation alone. Analyzing over a century of manufacturing data, he finds that technology-driven productivity gains initially increased jobs when demand was elastic but eventually reduced employment once markets became saturated. This suggests that geographic regions where AI targets unmet needs may experience job growth, while areas where AI serves already-saturated markets face potential displacement—a distinction crucial for understanding the uneven geographic distribution of AI career opportunities.\n\n\nRemote Work and Post-Pandemic Restructuring\nWith COVID-19 disrupting various business models, American corporations had to quickly restructure operations. Five years later, COVID-19 is no longer considered a national emergency, with various public and private sector organizations implementing return-to-office (RTO) mandates (Paulino et al. (2025)). Although the trend is currently favorable for RTO policies, “smaller companies, which often cannot compete on salary alone, are increasingly embracing remote work flexibility to attract and retain top talent” (Paulino et al. (2025)). Smaller companies can use remote work policies to their advantage, whether to lower their overhead costs or retain top talent without offering as much salary compared to in-office based roles.\n\n\nEmerging Tech Hubs and Geographic Concentration\nTech hubs seem to be emerging in droves across various parts of America. According to Chen, Levanon, and Sigelman ((2024)), although there are many smaller hubs forming, the Silicon Valley on the West Coast of the United States is still dominant. They explore the idea that there is a relationship between emerging tech hubs and established tech hubs: “the greatest determinant of migration is geographic proximity” (Chen, Levanon, and Sigelman (2024)). Another reason for the expansion of tech hubs in the United States is “likely motivated by quality of life and cost of living” (Chen, Levanon, and Sigelman (2024)). Although other tech hubs are spreading towards other regions in the United States, Silicon Valley residents shouldn’t see this as a threat, since growing tech talent causes a synergy that, as a result, both grows and redistributes tech employment.\n\n\nUrban-Rural Disparities in Remote Work Access\nGeographic location significantly shapes access to remote work opportunities, with important implications for employment equity. Paul ((2022)) analyzes pre-pandemic travel survey data and finds that rural workers prefer but rarely access work-from-home arrangements compared to urban counterparts, primarily due to inadequate broadband infrastructure and the geographic concentration of remote-compatible jobs in metropolitan areas. The study reveals that urban workers were nearly twice as likely to have remote work options, while rural workers—particularly Hispanic rural workers—faced compounded disadvantages in accessing virtual employment. These patterns suggest that AI careers requiring advanced digital infrastructure and remote work capabilities may concentrate in urban areas unless targeted policies address rural technology access gaps."
  },
  {
    "objectID": "introduction.html#methodology",
    "href": "introduction.html#methodology",
    "title": "Research Introduction: AI vs. Non-AI Careers",
    "section": "Methodology",
    "text": "Methodology\nWe undertook an investigation of job posting data from January to September 2024. Our tool, Lightcast, enabled us to standardize the many titles, locations, and employer information that populated the job postings. Through that process, we also conducted a feature analysis to understand the kinds of construction that are going on for jobs requiring artificial intelligence “in both a technical and a generative sense.” Overall, we read just over 22,000 job postings requiring AI-related skills.\nThen on the basic level, we conducted some summary and inferential statistics to understand the kinds of pay differences that might exist among postings for AI and non-AI jobs. On another level, we ran the postings through a random-forest regressor—a kind of model that uses certain given variables to try to explain or understand what is going on with the posting salaries. The model itself has 10 different versions, and each one assigns weights to the different given variables (such as the industry in which the job is located) to come up with a predicted salary. Results should shed light on the nature of pay for AI skills in the job market and how geographic and industrial factors influence compensation patterns."
  },
  {
    "objectID": "introduction.html#research-questions",
    "href": "introduction.html#research-questions",
    "title": "Research Introduction: AI vs. Non-AI Careers",
    "section": "Research Questions",
    "text": "Research Questions\nThis analysis addresses the following key questions:\n\nGeographic Distribution: Where are AI jobs concentrated, and how does this compare to non-AI career opportunities?\nRemote Work Trends: How have remote, hybrid, and on-site work arrangements evolved post-pandemic for AI versus non-AI positions?\nUrban-Rural Divide: What disparities exist between urban and rural areas in terms of AI job growth and compensation?\nCompensation Patterns: How do salaries for AI-related positions compare across different geographic regions and work arrangements?"
  },
  {
    "objectID": "introduction.html#structure-of-this-report",
    "href": "introduction.html#structure-of-this-report",
    "title": "Research Introduction: AI vs. Non-AI Careers",
    "section": "Structure of This Report",
    "text": "Structure of This Report\nThe remainder of this report is organized as follows: Section 2 presents our findings on remote work trends; Section 3 examines the urban-rural divide in AI employment; Section 4 discusses implications for job seekers, employers, and policymakers; and Section 5 concludes with recommendations for fostering inclusive growth in the AI economy."
  },
  {
    "objectID": "introduction.html#references",
    "href": "introduction.html#references",
    "title": "Research Introduction: AI vs. Non-AI Careers",
    "section": "References",
    "text": "References\n\n\nBessen, J. (2018): AI and Jobs: The Role of Demand, Working Paper, National Bureau of Economic Research.\n\n\nChen, Z., G. Levanon, and M. Sigelman. (2024): America’s Tech Hubs Are Multiplying: How Tech Powerhouses’ Diaspora Are Fueling the Rise of New Cities on the Talent Frontier,The Burning Glass Institute.\n\n\nPaul, J. A. (2022): “Work from home behaviors among u.s. Urban and rural residents,” Transportation Research Part A: Policy and Practice, 165, 254–69.\n\n\nPaulino, M., S. J. London, L. M. Giurge, R. W. Buell, and A. S. Gabriel. (2025): “Return-to-office mandates and workplace inequality: Implications for industrial-organizational psychology,” Industrial and Organizational Psychology,."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geographic & Remote Work Analysis",
    "section": "",
    "text": "This project analyzes job market trends in 2024, focusing on geographic and remote work analysis."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Geographic & Remote Work Analysis",
    "section": "",
    "text": "This project analyzes job market trends in 2024, focusing on geographic and remote work analysis."
  },
  {
    "objectID": "index.html#key-findings",
    "href": "index.html#key-findings",
    "title": "Geographic & Remote Work Analysis",
    "section": "Key Findings",
    "text": "Key Findings\n\nEmerging tech hubs are now leading in job growth, outpacing legacy hubs\nRemote work availability is highest in the Real Estate, Arts, and Finance sectors. Additionally, most of the remote job postings are from Alaska\nTop in-demand skills include…"
  },
  {
    "objectID": "index.html#explore-the-analysis",
    "href": "index.html#explore-the-analysis",
    "title": "Geographic & Remote Work Analysis",
    "section": "Explore the Analysis",
    "text": "Explore the Analysis\nResearch Introduction: Background and research questions Geographic Trends: EDA and Visualizations: Exploratory data analysis Tech Hub Analysis: Legacy vs emerging tech hubs Skill Gap Analysis: In-demand skills analysis Random Forest: Predictive modeling"
  },
  {
    "objectID": "index.html#about-this-project",
    "href": "index.html#about-this-project",
    "title": "Geographic & Remote Work Analysis",
    "section": "About This Project",
    "text": "About This Project\nDataset: Lightcast Job Postings (Jan-Sep 2024) Team: Group 09 Course: AD688 - Cloud Analytics"
  }
]