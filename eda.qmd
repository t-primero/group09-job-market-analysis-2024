---
title: "EDA"
format: html
execute:
  echo: true
  warning: false
---

```{python}
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("JobPostingsEDA").getOrCreate()
df = spark.read.option("header","true").option("inferSchema","true") \
    .option("multiLine","true").option("escape","\"") \
    .csv("data/raw/lightcast_job_postings.csv")
df.cache()
df.count()
```

```{python}
from pyspark.sql.functions import when, col, trim
from pyspark.sql.types import IntegerType, FloatType

cols_blank = ["CITY_NAME","STATE_NAME","REMOTE_TYPE","MIN_EDULEVELS",
              "SALARY","SALARY_FROM","SALARY_TO",
              "MIN_YEARS_EXPERIENCE","MAX_YEARS_EXPERIENCE"]
for c in cols_blank:
    if c in df.columns:
        df = df.withColumn(c, when(trim(col(c)) == "", None).otherwise(col(c)))

df = df.withColumn("REMOTE_TYPE", col("REMOTE_TYPE").cast(IntegerType())) \
       .withColumn("MIN_EDULEVELS", col("MIN_EDULEVELS").cast(IntegerType())) \
       .withColumn("SALARY", col("SALARY").cast(FloatType())) \
       .withColumn("SALARY_FROM", col("SALARY_FROM").cast(FloatType())) \
       .withColumn("SALARY_TO", col("SALARY_TO").cast(FloatType())) \
       .withColumn("MIN_YEARS_EXPERIENCE", col("MIN_YEARS_EXPERIENCE").cast(FloatType())) \
       .withColumn("MAX_YEARS_EXPERIENCE", col("MAX_YEARS_EXPERIENCE").cast(FloatType()))

df = df.withColumn(
    "REMOTE_TYPE",
    when(col("REMOTE_TYPE") == 1, "Remote")
    .when(col("REMOTE_TYPE") == 2, "Onsite")
    .when(col("REMOTE_TYPE") == 3, "Hybrid")
    .otherwise("Onsite")
)

df = df.withColumn(
    "MIN_EDULEVELS",
    when(col("MIN_EDULEVELS").isin(0,1,99), "Associate or lower")
    .when(col("MIN_EDULEVELS") == 2, "Bachelor")
    .when(col("MIN_EDULEVELS") == 3, "Master's")
    .when(col("MIN_EDULEVELS") == 4, "PhD")
    .otherwise("Associate or lower")
)

df = df.withColumn("CITY_NAME", when(col("CITY_NAME").isNull(), "No City").otherwise(col("CITY_NAME")))
df = df.withColumn("STATE_NAME", when(col("STATE_NAME").isNull(), "No State").otherwise(col("STATE_NAME")))

df = df.withColumn(
    "SALARY",
    when(col("SALARY").isNull() & col("SALARY_FROM").isNotNull() & col("SALARY_TO").isNotNull(),
         (col("SALARY_FROM") + col("SALARY_TO"))/2.0
    ).otherwise(col("SALARY"))
)
df.count()
```

```{python}
df.write.mode("overwrite").parquet("data/clean_job_postings.parquet")
```

```{python}
# Remote share by state
state_remote = df.groupBy("STATE_NAME","REMOTE_TYPE").count().orderBy("count", ascending=False)
state_remote.show(20, truncate=False)

# Top cities by postings
df.groupBy("CITY_NAME","STATE_NAME").count().orderBy("count", ascending=False).show(20, truncate=False)

# Salary by state
df.groupBy("STATE_NAME").avg("SALARY").orderBy("avg(SALARY)", ascending=False).show(20, truncate=False)
```

```{python}
from pyspark.sql.functions import col, count, sum as _sum, when, to_date, year, month, desc

# Parse posting date if available
date_col = "POSTED_DATE"
if date_col in df.columns:
    df = df.withColumn(date_col, to_date(col(date_col)))
    df = df.withColumn("YEAR", year(col(date_col))).withColumn("MONTH", month(col(date_col)))

# Flag AI vs non AI using title and description keywords
ai_patterns = ["ai","artificial intelligence","machine learning","ml","deep learning","nlp","computer vision","llm","genai","gen ai","data scientist","ml engineer"]
def mk_like(c):
    expr = None
    for p in ai_patterns:
        cond = col(c).rlike(f"(?i)\\b{p}\\b")
        expr = cond if expr is None else (expr | cond)
    return expr

ai_flag = None
if "TITLE" in df.columns:
    ai_flag = mk_like("TITLE")
if "JOB_DESCRIPTION" in df.columns:
    ai_desc = mk_like("JOB_DESCRIPTION")
    ai_flag = ai_desc if ai_flag is None else (ai_flag | ai_desc)

if ai_flag is not None:
    df = df.withColumn("IS_AI_ROLE", when(ai_flag, 1).otherwise(0))
else:
    df = df.withColumn("IS_AI_ROLE", when(col("TITLE").isNotNull(), 0).otherwise(0))
```

```{python}
from pyspark.sql.functions import desc
```

```{python}
# Remote share by state
state_remote = df.groupBy("STATE_NAME","REMOTE_TYPE").count().orderBy("count", ascending=False)
state_remote.show(30, truncate=False)

# Top states by postings
state_totals = df.groupBy("STATE_NAME").count().orderBy("count", ascending=False)
state_totals.show(30, truncate=False)

# AI vs non AI counts by state
from pyspark.sql.functions import count, sum as _sum, avg
ai_state = df.groupBy("STATE_NAME").agg(
    _sum("IS_AI_ROLE").alias("ai_posts"),
    (count("*") - _sum("IS_AI_ROLE")).alias("non_ai_posts")
).orderBy("ai_posts", ascending=False)
ai_state.show(30, truncate=False)

# Monthly trend of remote vs onsite overall
if "YEAR" in df.columns and "MONTH" in df.columns:
    monthly_remote = df.groupBy("YEAR","MONTH","REMOTE_TYPE").count().orderBy("YEAR","MONTH","REMOTE_TYPE")
    monthly_remote.show(60, truncate=False)

# Salary by state for AI vs non AI
if "SALARY" in df.columns:
    sal_by_state_ai = df.groupBy("STATE_NAME","IS_AI_ROLE").agg(avg("SALARY").alias("avg_salary")).orderBy("avg_salary", ascending=False)
    sal_by_state_ai.show(30, truncate=False)
    ```